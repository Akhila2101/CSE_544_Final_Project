# -*- coding: utf-8 -*-
"""ProbStats_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BvmEiGepXyfq2iyqRN2zza--Y9dwUj9U

# Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
from scipy.stats import gamma
import math
import datetime
import warnings
import random
import math
from itertools import permutations
from sklearn.linear_model import LinearRegression
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

"""# Load Data"""

total_cases=pd.read_csv("/content/drive/MyDrive/Prob Stats Project Data/cases.csv")
total_vaccinations=pd.read_csv("/content/drive/MyDrive/Prob Stats Project Data/vaccinations.csv")

total_vaccinations.rename(columns={"Location":"state"},inplace=True)
total_cases.rename(columns={"submission_date":"Date"},inplace=True)

total_cases.head()

def filter_data(data,states,columns):
  data=data.loc[data["state"].isin(states)]
  return data[columns].reset_index(drop=True)

states=["AL","CA"]
columns=["Date","state","new_case","new_death"]
cases=filter_data(total_cases,states,columns)
columns=["Date","state","Administered","Admin_Per_100K"]
vaccinations=filter_data(total_vaccinations,states,columns)

vaccinations.head()

cases.head()

"""# Data Preprocessing"""

def remove_nulls(data):
  data=data[data.select_dtypes(include=[np.number]).ge(0).all(1)]
  return data.dropna(axis=0,how="any")

def convert_date(data):
  data['Date'] =  pd.to_datetime(data['Date'], format=r"%m/%d/%Y")
  return data

def sort_seperate_states(data):
  a,b=data.loc[data["state"]=="AL"],data.loc[data["state"]=="CA"]
  a.sort_values(by="Date",ignore_index=True,inplace=True)
  b.sort_values(by="Date",ignore_index=True,inplace=True)
  return a,b

def getMonth(data):
  data["month"]=pd.DatetimeIndex(data['Date']).month
  return data

from collections import Counter
import matplotlib.pyplot as plt

def remove_outliers(value,min_threshold,max_threshold):
  if value < min_threshold or value > max_threshold:
    return np.nan
  return value

def remove_all_outliers(data,columns):
  outlier_index = []
  for column in columns:
    temp_data=data[column]
    print(temp_data)
    q1=np.percentile(temp_data,25)
    q3=np.percentile(temp_data,75)
    iqr=q3-q1
    min_threshold,max_threshold=q1-(1.5*iqr),q3+(1.5*iqr)
    #Determine a list of indices of outliers for feature col
    outlier_list_col = data[(data[column] > max_threshold) | (data[column] < min_threshold)]
    print("Outliers in Column "+str(column) + " : " + str(len(outlier_list_col)))
    data[column] = data.apply(lambda x: remove_outliers(x[column],min_threshold, max_threshold), axis=1)
    plt.scatter(data['Date'],data[column])
    plt.scatter(outlier_list_col['Date'],outlier_list_col[column])
    plt.rcParams["figure.figsize"] = (10,6)
    plt.show()
  return remove_nulls(data).reset_index(drop=True)

def get_daily_data(data):
  data.sort_values(by="Date",ignore_index=True,inplace=True)
  t1=data["Administered"][0]
  t2=data["Admin_Per_100K"][0]
  data['Count'] = data.Administered - data.Administered.shift(1)
  data["Count_Per_100K"]=data.Admin_Per_100K-data.Admin_Per_100K.shift(1)
  data["Count"][0]=t1
  data["Count_Per_100K"]=t2
  return data

import seaborn as sns
def transform_data(data):
  data=remove_nulls(data)
  data=convert_date(data)
  data=getMonth(data)
  data_alaska,data_cali=sort_seperate_states(data)
  data_alaska=remove_all_outliers(data_alaska,["new_case","new_death"])
  data_cali=remove_all_outliers(data_cali,["new_case","new_death"])
  return data_alaska,data_cali

def transform_data_TaskA(data):
  data=remove_nulls(data)
  #data=remove_all_outliers(data,["new_case","new_death"])
  data=convert_date(data)
  data=getMonth(data)
  data_alaska,data_cali=sort_seperate_states(data)
  return data_alaska,data_cali

cases_alaska,cases_cali=transform_data(cases)





vaccinations.head()

def transform_data_Vacc(data):
  data=remove_nulls(data)
  data=convert_date(data)
  data_alaska,data_cali=sort_seperate_states(data)
  data_alaska=get_daily_data(data_alaska)
  data_cali=get_daily_data(data_cali)
  data_alaska=remove_all_outliers(data_alaska,["Count","Count_Per_100K"])
  data_cali=remove_all_outliers(data_cali,["Count","Count_Per_100K"])
  return data_alaska,data_cali

vaccination_alaska,vaccination_cali=transform_data_Vacc(vaccinations)

vaccination_alaska.head()

vaccination_cali.head()

"""# Task - A"""

def compute_walds_statistc(x,y):
  n=len(x)
  x_mean, y_mean, x_var, y_var = np.mean(x), np.mean(y), np.mean(x), np.mean(y)
  sd = (x_mean/n)**0.5
  #sd = np.sqrt(x_var/len(x))
  w = (x_mean - y_mean)/sd
  print("u0: "+ str(y_mean))
  print("u_hat: "+ str(x_mean))
  print("Standard error: "+ str(sd))
  print("walds statistic: "+ str(w))
  return w
def walds_test(a,b,alpha):
  w=compute_walds_statistc(a,b)
  if abs(w) < st.norm.ppf(1-(alpha/2)):
    return False
  else:
    return True

def compute_2Sampled_walds_statistc(x,y):
  n1=len(x)
  n2=len(y)
  x_mean, y_mean, x_var, y_var = np.mean(x), np.mean(y), np.mean(x), np.mean(y)
  sd = (x_var/n1 + y_var/n2)**0.5
  w = (x_mean - y_mean)/sd
  print("u0: "+ str(y_mean))
  print("u_hat: "+ str(x_mean))
  print("Standard error: "+ str(sd))
  print("walds statistic: "+ str(w))
  return w
def walds_2Sampled_test(a,b,alpha):
  w=compute_2Sampled_walds_statistc(a,b)
  if abs(w) < st.norm.ppf(1-(alpha/2)):
    return False
  else:
    return True

import scipy.stats
def compute_2Sampled_t_statistic(x,y):
  n,m=len(x),len(y)
  x_mean, y_mean, x_var, y_var = np.mean(x), np.mean(y), np.var(x), np.mean(y)
  denom=math.sqrt((x_var/n) + (y_var/m))
  corrected_variance_march = np.sum(np.square(x - x_mean))/(len(x)-1)
  corrected_variance_feb =  np.sum(np.square(y - y_mean))/(len(y)-1)
  t=(x_mean-y_mean)/np.sqrt(corrected_variance_march/len(x) + corrected_variance_feb/len(y))
  print("u0: "+ str(y_mean))
  print("u_hat: "+ str(x_mean))
  print("Standard error: "+ str(denom))
  print("T statistic: "+ str(t))
  return t
  
def t_2Sampled_test(a,b,alpha):
  t=compute_2Sampled_t_statistic(a,b)
  if abs(t)<scipy.stats.t.ppf(q=1-.05/2,df=594):
    return False
  else:
    return True


def variance(arr):
    square_sum = 0
    mean = arr.mean()
    n = len(arr)
    for i in arr:
        square_sum = square_sum + (i -mean)*(i-mean)
    return square_sum/(n-1)

import scipy.stats
def compute_t_statistic(x,y):
  n,m=len(x),len(y)
  x_mean, y_mean= np.mean(x), np.mean(y)
  denom = np.sqrt(variance(x)/len(x))
  t=(x_mean-y_mean)/(denom)
  print("u0: "+ str(y_mean))
  print("u_hat: "+ str(x_mean))
  print("Standard error: "+ str(denom))
  print("T statistic: "+ str(t))
  return t
  
def t_test(a,b,alpha):
  t=compute_t_statistic(a,b)
  print(scipy.stats.t.ppf(q=1-.05/2,df=29))
  if abs(t)<scipy.stats.t.ppf(q=1-.05/2,df=29):
    return False
  else:
    return True


def variance(arr):
    square_sum = 0
    mean = arr.mean()
    n = len(arr)
    for i in arr:
        square_sum = square_sum + (i -mean)*(i-mean)
    return square_sum/(n-1)

def compute_z_statistic(x,y):
  n,m=len(x),len(y)
  x_mean,y_mean,x_var,y_var=np.mean(x),np.mean(y),np.var(x),np.var(y)
  deno=(((x_var/n))**0.5)
  Z = (x_mean - y_mean)/deno
  print("u0: "+ str(y_mean))
  print("u_hat: "+ str(x_mean))
  print("Standard error: "+ str(deno))
  print("Z statistic: "+ str(Z))
  return Z
def ztest(a,b,alpha):
  z=compute_z_statistic(a,b)
  if abs(z)<st.norm.ppf(1-(alpha/2)):
    return False
  else:
    return True

cases_alaska,cases_cali=transform_data_TaskA(cases)

"""Hypothesis tests for California"""

feb_start_date , feb_end_date = '2021-02-01', '2021-02-28'
march_start_date, march_end_date = '2021-03-01', '2021-03-31'
condition = (cases_cali['Date'] >= feb_start_date) & (cases_cali['Date'] <= feb_end_date)
CA_feb_data = cases_cali.loc[condition]

condition = (cases_cali['Date'] >= march_start_date) & (cases_cali['Date'] <= march_end_date)
CA_march_data = cases_cali.loc[condition]


CA_feb_data=pd.DataFrame(CA_feb_data)
CA_march_data=pd.DataFrame(CA_march_data)
CA_march_data.shape

"""Hypothesis test on mean of #Deaths different for March and Feb 21"""

walds_test(CA_march_data['new_death'],CA_feb_data['new_death'],0.05)

ztest(CA_march_data['new_death'],CA_feb_data['new_death'],0.05)

t_test(CA_march_data['new_death'],CA_feb_data['new_death'],0.05)

"""Hypothesis test on mean of #Cases different for March and Feb 21"""

walds_test(CA_march_data['new_case'],CA_feb_data['new_case'],0.05)

t_test(CA_march_data['new_case'],CA_feb_data['new_case'],0.05)

ztest(CA_march_data['new_case'],CA_feb_data['new_case'],0.05)

"""## Result of Walds 1 sample testing for mean of cases and death in CA:

Null hypothesis (H0):

Mean of daily cases/deaths in CA for March’21 is different from the corresponding mean of daily values for Feb’21

Alternate hypothesis(H1):

Mean of March'21 cases/deaths in CA is same as mean of Feb'21 cases/deaths

Procedure :

We took the u0(guess value) as mean of Feb'21 cases/deaths and alpha = 0.05 as given in documentation and the MLE estimator for mean of March'21 cases/deaths becomes the smaple mean of  cases/deaths .The standard error of the estimator is calculated in above walds function.
Result:

As the w value for mean of March'21 death =89.471 which is greater than 1.96 we are Accepting the NULL hypothesis.

As the w value for mean of March'21 cases =538.925 which is greater than 1.96 we are Accepting the NULL hypothesis.

## Result of Z testing for mean of cases and death for California (CA)

Null hypothesis (H0):

Mean of March'21 cases/deaths in CA different from Mean of Feb'21 cases/deaths in CA.

Alternate hypothesis(H1):

Mean of March'21 cases/deaths in CA is same as Mean of Feb'21 cases/deaths in CA.

Result /Inference:

As the Calculated Z value for mean of March'21 deaths in CA = 10.616 which is greater than 1.96 we are accepting the NULL hypothesis.

As the Calculated Z value for mean of March'21 cases in CA = 30.323 which is greater than 1.96 we are accepting the NULL hypothesis.

Is Test Applicable ?

The main Assumptions of Z-test are the sample size has to be large or the sample data has to be normally dustributed.Here we can clearly see sample size is around 30 which satisfies the size threshold and through CLT data behaves as normal .

Hence ,We can conclude the Z Test is applicable on given dataset.

## Result of T 1 sample testing for mean of cases/death in CA:

Null hypothesis (H0):

Mean of March'21 cases/deaths is different from mean of Feb'21 cases/deaths in CA.

Alternate hypothesis(H1):

Mean of March'21 cases/deaths is same as mean of Feb'21 cases/deaths in CA.

Procedure :

We have taken the alpha = 0.05,n =30 as we took 30 days of data as given in task and calculated the numerator and denominator of T in the above T_one_sample_testing function

Result:

As the calculated value for mean of March'21 deaths in CA = 10.443 which is greater than T value 2.045 we are accepting the NULL hypothesis.

As the calculated value for mean of last week cases = 29.830 which is greater than T value 2.045 we are accepting the NULL hypothesis.

Is T-1 sample Test Applicable ?

The main assumption of T-test is the data is normally distributed, here it is not but as size n is around 30 and through CLT data behaves as T distribution.

Hypothesis tests for Alaska
"""

feb_start_date , feb_end_date = '2021-02-01', '2021-02-28'
march_start_date, march_end_date = '2021-03-01', '2021-03-31'
condition = (cases_alaska['Date'] >= feb_start_date) & (cases_alaska['Date'] <= feb_end_date)
AL_feb_data = cases_alaska.loc[condition]

condition = (cases_alaska['Date'] >= march_start_date) & (cases_alaska['Date'] <= march_end_date)
AL_march_data = cases_alaska.loc[condition]


Al_feb_data=pd.DataFrame(AL_feb_data)
AL_march_data=pd.DataFrame(AL_march_data)
AL_feb_data.shape

walds_test(AL_march_data['new_death'],Al_feb_data['new_death'],0.05)

ztest(AL_march_data['new_death'],Al_feb_data['new_death'],0.05)

t_test(AL_march_data['new_death'],Al_feb_data['new_death'],0.05)

"""Hypothesis test on mean of #Cases different for March and Feb 21"""

walds_test(AL_march_data['new_case'],Al_feb_data['new_case'],0.05)

ztest(AL_march_data['new_case'],Al_feb_data['new_case'],0.05)

t_test(AL_march_data['new_case'],Al_feb_data['new_case'],0.05)

"""## **Result of Walds 1 sample testing for mean of cases and death in Alaska(AL):**

Null hypothesis (H0):

Mean of daily cases/deaths in AL for March’21 is different from the corresponding mean of daily values for Feb’21

Alternate hypothesis(H1):

Mean of March'21 cases/deaths in AL is same as mean of Feb'21 cases/deaths

Procedure :

We took the u0(guess value) as mean of Feb'21 cases/deaths and alpha = 0.05 as given in documentation and the MLE estimator for mean of March'21 cases/deaths becomes the smaple mean of  cases/deaths .The standard error of the estimator is calculated in above walds function.
Result:

As the w value for mean of March'21 death =34.325 which is greater than 1.96 we are Accepting the NULL hypothesis.

As the w value for mean of March'21 cases =100.517 which is greater than 1.96 we are Accepting the NULL hypothesis.

## Result of Z testing for mean of cases and death for Alaska (AL)

Null hypothesis (H0):

Mean of March'21 cases/deaths in AL different from Mean of Feb'21 cases/deaths in AL.

Alternate hypothesis(H1):

Mean of March'21 cases/deaths in AL is same as Mean of Feb'21 cases/deaths in AL.

Result /Inference:

As the Calculated Z value for mean of March'21 deaths in AL = 27.235 which is greater than 1.96 we are accepting the NULL hypothesis.

As the Calculated Z value for mean of March'21 cases in CA = 3.499 which is greater than 1.96 we are accepting the NULL hypothesis.

Is Test Applicable ?

The main Assumptions of Z-test are the sample size has to be large or the sample data has to be normally dustributed.Here we can clearly see sample size is around 30 which satisfies the size threshold and through CLT data behaves as normal .

Hence ,We can conclude the Z Test is applicable on given dataset.

## Result of T 1 sample testing for mean of cases/death in AL:

Null hypothesis (H0):

Mean of March'21 cases/deaths is different from mean of Feb'21 cases/deaths in AL.

Alternate hypothesis(H1):

Mean of March'21 cases/deaths is same as mean of Feb'21 cases/deaths in AL.

Procedure :

We have taken the alpha = 0.05,n =30 as we took 30 days of data as given in task and calculated the numerator and denominator of T in the above T_one_sample_testing function

Result:

As the calculated value for mean of March'21 deaths in AL = 26.792 which is greater than T value 2.045 we are accepting the NULL hypothesis.

As the calculated value for mean of last week cases = 3.442 which is greater than T value 2.045 we are accepting the NULL hypothesis.

Is T-1 sample Test Applicable ?

The main assumption of T-test is the data is normally distributed, here it is not but as size n is around 30 and through CLT data behaves as T distribution.

Wald's 2 sample test
"""

walds_2Sampled_test(AL_march_data['new_death'],Al_feb_data['new_death'],0.05)

walds_2Sampled_test(AL_march_data['new_case'],Al_feb_data['new_case'],0.05)

walds_2Sampled_test(CA_march_data['new_death'],CA_feb_data['new_death'],0.05)

walds_2Sampled_test(CA_march_data['new_case'],CA_feb_data['new_case'],0.05)

"""## **Result of Walds 2 sampled test for mean of cases and death in Alaska(AL) and California(CA):**

Null hypothesis (H0):

Mean of daily cases/deaths in AL/CA for March’21 is different from the corresponding mean of daily values for Feb’21

Alternate hypothesis(H1):

Mean of March'21 cases/deaths in AL/CA is same as mean of Feb'21 cases/deaths

Procedure :

We took the u0(guess value) as mean of Feb'21 cases/deaths and alpha = 0.05 as given in documentation and the MLE estimator for mean of March'21 cases/deaths becomes the smaple mean of  cases/deaths .The standard error of the estimator is combination of the standard error of both the time frame data March and Feb.

Result:

As the w statistic value for deaths in AL =17.1367 which is greater than 1.96 we are Accepting the NULL hypothesis.

As the w  statistic value for cases in AL =59.526 which is greater than 1.96 we are Accepting the NULL hypothesis.


As the w statistic value for death in CA =48.730 which is greater than 1.96 we are Accepting the NULL hypothesis.

As the w statistic value for cases in CA =267.295 which is greater than 1.96 we are Accepting the NULL hypothesis.

Unpaired 2-sample T-test
"""

t_2Sampled_test(AL_march_data['new_death'],Al_feb_data['new_death'],0.05)

t_2Sampled_test(AL_march_data['new_case'],Al_feb_data['new_case'],0.05)

t_2Sampled_test(CA_march_data['new_case'],CA_feb_data['new_case'],0.05)

t_2Sampled_test(CA_march_data['new_death'],CA_feb_data['new_death'],0.05)



"""## Result of T 2 sample unpaired testing for mean of cases and death for CA/AL

Null hypothesis (H0):

Mean of March'21 cases/deaths in CA/AL is different from Mean of Feb'21 cases/deaths.

Alternate hypothesis(H1):

mean of March'21 cases/deaths in CA/AL is same as mean of Feb'21 cases/deaths.

Procedure :

We have taken the alpha = 0.05 n=30, m=28 as given in task and calculated the numerator and denominator of T value in the above t_2Sampled_test function .

Result:

As the T-Statistic value for cases in CA = 7.767 which is greater than 2.18 we are accpeting the NULL hypothesis.

As the T-Statistic value cases in AL = 2.760 which is less than 2.18 we are accpeting the NULL hypothesis.

As the T-statistic value for deaths in AL = 9.343 which is greater than 2.18 we are accepting the NULL hypothesis.

As the T-statistic value for deaths in CA = 4.811 which is greater than 2.18 we are accepting the NULL hypothesis.

# Task - B
"""

cases_alaska,cases_cali=transform_data(cases)

cases_alaska_b=cases_alaska.loc[(cases_alaska["month"].isin([10,11,12]))& (cases_alaska["Date"].dt.year==2021)].reset_index(drop=True)
cases_cali_b=cases_cali.loc[(cases_cali["month"].isin([10,11,12]))&(cases_cali["Date"].dt.year==2021)].reset_index(drop=True)

cases_cali_b

"""## Permutation Test Function

The permutation test is used to check whether two data samples follow the same distribution

**Result of Permutation test for two state cases**

**Null hypothesis (H0):**
Distribution of state Alaska cases equals distribution of state California cases

**Alternate hypothesis(H1):**
Distribution of state Alaska cases not equals to distribution of  state California cases



**Procedure :**
We permutted all Alaska state records and California records data 1000 ways.
We take alpha = 0.05, as stated in the documentation, and calculate it.
  

**Is the Permutation Test appliable ?**

There as no assumptions under KS test, hence the test is applicable
"""

def generate_data(l,a,b,T_obs):
  print("T_obs",T_obs)
  def find_xmean(row):
    return sum(row["X"])/len(row["X"])
  def find_ymean(row):
    return sum(row["Y"])/len(row["Y"])
  def finddiff(row):
    return round(abs(row["X_mean"]-row["Y_mean"]),2)
  def solve(row):
    return 1 if row["T = |X_mean - Y_mean |"] > T_obs else 0
  n=len(l)
  X,Y=[],[]
  for i in range(n):
    X.append(l[i][:a])
    Y.append(l[i][a:])
  data=pd.DataFrame()
  
  data["X"],data["Y"]=X,Y
  data["X_mean"] = data.apply(find_xmean, axis = 1)
  data["Y_mean"] = data.apply(find_ymean, axis = 1)
  data["T = |X_mean - Y_mean |"] = data.apply(finddiff,axis = 1)
  data["I (T > T_obs)"] = data.apply(solve,axis = 1)
  return data

def perm_generator(seq):
  seen = set()
  length = len(seq)
  while True:
      perm = tuple(random.sample(seq, length))
      if perm not in seen:
          seen.add(perm)
          yield perm

def p_value(data):
  return data.iloc[:,-1].sum()/len(data)

def get_p_value(data_alaska,data_cali,n):
  a,b=len(data_alaska),len(data_cali)
  data=data_alaska+data_cali
  rand_perms = perm_generator(data)
  permlist= [next(rand_perms) for _ in range(n)]
  permlist=list(map(list,permlist))
  T_obs = abs(np.mean(data_alaska)-np.mean(data_cali))
  data=generate_data(permlist,a,b,T_obs)
  return p_value(data)

def permutation_test(X,Y,alpha):
  a,b = len(X),len(Y)
  n=min(math.factorial(a+b),1000)
  p=get_p_value(X,Y,n)
  print("P Value:",p)
  if p < alpha :
    return False
  else:
    return True

permutation_test(list(cases_alaska_b["new_case"]),list(cases_cali_b["new_case"]),0.05)

permutation_test(list(cases_alaska_b["new_death"]),list(cases_cali_b["new_death"]),0.05)

"""## K-S One Sample Test#"""

def eCDF_value(state1):
    state_l = len(state1)
    l = sorted(state1)
    delta = .1
    X = []
    f_x = [0]
    for i in range(0,state_l):
        X = X + [l[i]]
        f_x = f_x + [f_x[len(f_x)-1]+(1/state_l)]
    f_x = f_x + [1]
    return X,f_x

def one_sample_ks_test(X,f_x, CDF, parameter,distribution):
    maximum_value = -1000
        
    list = np.zeros((len(X),4))
    for i in range(len(list)):
        list[i,0] = f_x[i]
        list[i,1] = f_x[i+1]

        F_x = CDF(parameter, X[i],distribution)
        list[i,2] = abs(list[i,0] - F_x)
        list[i,3] = abs(list[i,1] - F_x)
        cmax = max(list[i,2], list[i,3])
        if cmax > maximum_value:
            maximum_value = cmax
        
        
    return maximum_value

"""##KS test for distribution of daily cases#"""

# Obtaining eCDF for state Alaska
state1_data =cases_alaska_b["new_case"]
x,f_x  = eCDF_value(state1_data)

from scipy.stats import poisson
from scipy.stats import geom
from scipy.stats import binom

def CDF_Distribution(parameter, x,distribution):
  if distribution=='Poisson':
    return poisson.cdf(x, parameter)
  elif distribution=='geometric':
    return geom.cdf(x, parameter)
  elif distribution=='binomial':
    return binom.cdf(x, parameter[0], parameter[1])

def MME_Distribution(x ,distribution):
  if distribution=='Poisson':
    return np.mean(x)
  elif distribution=='geometric':
    samplemean=np.mean(x)
    estimator=1/samplemean
    return estimator
  elif distribution=='binomial':
    samplemean=np.mean(x)
    variance = np.var(x)
    estimated_p= 1-(variance/samplemean)
    estimated_n=samplemean/estimated_p
    return estimated_p,estimated_n

"""***KS-test one sample test on cases***

###  Result of 1 sample KS test for for the last three months covid cases of 2021 with Poisson distribution###

**Null hypothesis (H0):** 
Distribution of Oct-Dec 2021 cases equals poisson distribution
**Alternate hypothesis(H1):**
Distribution of Oct-Dec 2021 cases not equals poisson distribution

**Procedure :**
We  obtained the parameters for the Poisson distribution  using MME on the second state data.
We take c = 0.05 as reported in the literature and calculate the maximum difference in the CDF of the distributions at all  points.

**Is  KS testing applicable?**
There are no assumptions in the KS test, so the test is applicable

**Poisson distribution :**
"""

lambda_value=MME_Distribution(cases_cali_b["new_case"],'Poisson')
print(' Poisson distribution parameter lambda : ', lambda_value)

KS_statistical_value= one_sample_ks_test(x , f_x , CDF_Distribution, lambda_value,'Poisson')

print('KS test statistical value : ', KS_statistical_value)

"""## **Result**

Since the statistical value of the KS test is ***1.000000000000001***, greater than 0.05, we **reject** the null hypothesis.

Oct-Dec 2021 data for the second state does not follow the poisson distribution.

**Geometric distribution :**

###  Result of 1 sample KS test for for the last three months of 2021 with Geometric distribution###

**Null hypothesis (H0):** 
Distribution of Oct-Dec 2021 cases equals geometric distribution
**Alternate hypothesis(H1):**
Distribution of Oct-Dec 2021 cases not equals geometric distribution

**Procedure :**
We  obtained the parameters for the Geometric distribution  using MME on the second state data.
We take c = 0.05 as reported in the literature and calculate the maximum difference in the CDF of the distributions at all  points.
"""

p_value=MME_Distribution(cases_cali_b["new_case"],'geometric')
print(' Geometric distribution parameter p : ', p_value)

KS_statistical_value_g= one_sample_ks_test(x , f_x , CDF_Distribution, p_value,'geometric')

print('KS test statistical value : ', KS_statistical_value_g)

"""## **Result**

Since the statistical value of the KS test is ***0.7038***, greater than 0.05, we **reject** the null hypothesis.

Oct-Dec 2021 data for the second state does not follow the geometric distribution.

**Binomial distribution :**
"""

n,p =MME_Distribution(cases_cali_b["new_case"],'binomial')
print(' Binomial distribution parameter n & p : ', n,p )

KS_statistical_value_b= one_sample_ks_test(x , f_x , CDF_Distribution, [n,p],'binomial')

print('KS test statistical value : ', KS_statistical_value_b)

"""## **Result**

Since the statistical value of the KS test is ***1***, greater than 0.05, we **reject** the null hypothesis.

Oct-Dec 2021 data for the second state does not follow the binomial distribution.

##KS test for daily death distribution#

###  Result of 1 sample KS test for for the last three months covid deaths of 2021 with Poisson distribution###

**Null hypothesis (H0):** 
Distribution of Oct-Dec 2021 deaths equals poisson distribution

**Alternate hypothesis(H1):**
Distribution of Oct-Dec 2021 deaths not equals poisson distribution

**Procedure :**
We  obtained the parameters for the Poisson distribution  using MME on the second state data.
We take c = 0.05 as reported in the literature and calculate the maximum difference in the CDF of the distributions at all  points.

**Is  KS testing applicable?**
There are no assumptions in the KS test, so the test is applicable
"""

# Obtaining eCDF for Alaska
state1_data_deaths =cases_alaska_b["new_death"]
x_death,f_x_death  = eCDF_value(state1_data)

"""**Poisson distribution :**"""

lambda_value=MME_Distribution(cases_cali_b["new_death"],'Poisson')
print(' Poisson distribution parameter lambda : ', lambda_value)

KS_statistical_value_deaths= one_sample_ks_test(x_death , f_x_death , CDF_Distribution, lambda_value,'Poisson')

print('KS test statistical value : ', KS_statistical_value_deaths)

"""## **Result**

Since the statistical value of the KS test is ***0.99***, greater than 0.05, we **reject** the null hypothesis.

Oct-Dec 2021 data for the second state deaths data does not follow the poisson distribution.

**Geometric distribution :**
"""

p_value=MME_Distribution(cases_cali_b["new_death"],'geometric')
print(' Geometric distribution parameter p : ', p_value)

KS_statistical_value_g_death= one_sample_ks_test(x_death , f_x_death, CDF_Distribution, p_value,'geometric')

print('KS test statistical value : ', KS_statistical_value_g_death)

"""## **Result**

Since the statistical value of the KS test is ***0.908***, greater than 0.05, we **reject** the null hypothesis.

Oct-Dec 2021 data for the second state deaths data does not follow the geometric distribution.

**Binomial distribution :**
"""

n,p =MME_Distribution(cases_cali_b["new_death"],'binomial')
print(' Binomial distribution parameter n & p : ', n,p )

KS_statistical_value_b_death= one_sample_ks_test(x_death , f_x_death, CDF_Distribution, [n,p],'binomial')

print('KS test statistical value : ', KS_statistical_value_b_death)

"""## **Result**

Since the statistical value of the KS test is ***1.0***, greater than 0.05, we **reject** the null hypothesis.

Oct-Dec 2021 data for the second state deaths data does not follow the binomial distribution.

## **KS 2-sample Test**

**KS test for distribution of daily cases**

**Result of 2 sample KS test for the equality of distributions between the two states(cases)**


**Null hypothesis (H0):**
Distribution of Oct-Dec 2021 cases of state Alaska equals  distribution of Oct-Dec 2021 cases of state California.

**Alternate hypothesis(H1):**
Distribution of Oct-Dec 2021 cases of state Alaska not equals distribution of Oct-Dec 2021 cases of state California.

**Procedure :**
We take c = 0.05 as reported in the literature and calculate the maximum difference in the CDF of the distributions at all points.

**Is the KS Test appliable ?**
There as no assumptions under KS test, hence the test is applicable
"""

def eCDF_value(state1):
    state_1 = len(state1)
    listvalues = sorted(state1)
    value=1/(state_1)
    X = []
    f_x = []
    cdf=value

    for i in range(0,state_1):
        X = X + [listvalues[i]]
        f_x = f_x + [cdf]
        cdf+=value

    return X,f_x

def ks_test(state1,state2,X1, f_X1, X2, f_X2):
  i=0
  j=0
  maxi1=0
  while i<len(state1) and j<len(state2):
    if state1[i] < state2[j]:
      if j==0:
        F_x_cap=0
      else:
        F_x_cap = f_X2[X2.index(state2[j-1])]
      if i==0:
        F_y_left=0
      else:
        F_y_left = f_X1[X1.index(X1[i-1])]
      if i==len(X1)-1:
        F_y_right=1
      else:
        F_y_right=f_X1[X1.index(state1[i])]
      maxi_temp=max(abs(F_x_cap-F_y_left),abs(F_x_cap-F_y_right))
      if maxi_temp>maxi1:
        maxi1=maxi_temp
        index=i
      i+=1
    else:
      j+=1

  print("K-S statistic is " + str(maxi1)+" and point of max difference is "+str(state1[index]))
  return maxi1

# Obtaining eCDF for cases
state1_data = cases_alaska_b["new_case"]

state2_data = cases_cali_b["new_case"]

X1, f_X1 = eCDF_value(state1_data)
X2, f_X2 = eCDF_value(state2_data)


KS_p_value_deaths = ks_test(state1_data.tolist(), state2_data.tolist(),X1, f_X1, X2, f_X2)

print('KS statistic : ', KS_p_value_deaths)
plt.figure('K-S test' , figsize=(10,7))
plt.xlabel("K-S 2 Sample Test")
plt.ylabel('Cumulative distribution frequency')
    
plt.step(X1, f_X1, where='post', label="Alaska")
plt.step(X2, f_X2, where='post', label="California")
plt.legend()
plt.show()

"""## **Result**

Since the statistical value of the KS test is ***0.978***, greater than 0.05, we **reject** the null hypothesis.

 The distribution of Oct-Dec 2021 cases of state Alaska does not equal the distribution of Oct-Dec 2021 cases of state California.

**KS test for distribution of daily deaths**

**Result of 2 sample KS test for the equality of distributions between the two states(deaths)**


**Null hypothesis (H0):**
Distribution of Oct-Dec 2021 deaths of state Alaska equals  distribution of Oct-Dec 2021 deaths of state California.

**Alternate hypothesis(H1):**
Distribution of Oct-Dec 2021 deaths of state Alaska not equals distribution of Oct-Dec 2021 deaths of state California.

**Procedure :**
We take c = 0.05 as reported in the literature and calculate the maximum difference in the CDF of the distributions at all points.
"""

# Obtaining eCDF for deaths
state1_data = cases_alaska_b["new_death"]

state2_data = cases_cali_b["new_death"]

X1, f_X1 = eCDF_value(state1_data)
X2, f_X2 = eCDF_value(state2_data)


KS_p_value_deaths = ks_test(state1_data.tolist(), state2_data.tolist(),X1, f_X1, X2, f_X2)

print('KS statistic : ', KS_p_value_deaths)
plt.figure('K-S test' , figsize=(10,7))
plt.xlabel("K-S 2 Sample Test")
plt.ylabel('Cumulative distribution frequency')
    
plt.step(X1, f_X1, where='post', label="Alaska")
plt.step(X2, f_X2, where='post', label="California")
plt.legend()
plt.show()

"""## **Result**

Since the statistical value of the KS test is ***1.00***, greater than 0.05, we **reject** the null hypothesis.

 The distribution of Oct-Dec 2021 deaths of state Alaska does not equal the distribution of Oct-Dec 2021 deaths of state California.

# Task C
"""

# This function is to merge the data from both states adding the cases and deaths
def merge(cases_alaska,cases_cali):
  cases1=cases_alaska.drop(["state"],axis=1)
  cases2=cases_cali.drop(["state"],axis=1)
  cases=pd.merge(cases1, cases2, how ='inner', on =["Date"])
  cases["Cases"]=cases["new_case_x"]+cases["new_case_y"]
  cases["Deaths"]=cases["new_death_x"]+cases["new_death_y"]
  return cases.drop(["new_case_x","new_case_y","new_death_x","new_death_y","month_x","month_y"],axis=1)

cases=merge(cases_alaska,cases_cali)

#getting the june data
june_data=cases.loc[(cases["Date"].dt.month==6) & (cases["Date"].dt.year==2020)].reset_index(drop=True)

lambd=np.mean(june_data["Cases"])

#This function generate the parameters for posterior Gamma Distribution
def generate_posterior(exp1,exp2,lambda1,lambda_2):
    return exp1+exp2,lambda1+lambda2

lambd= np.mean(june_data["Cases"])
print(' MME of poisson distributed data ', lambd)
exp_lambda = 1/lambd 
print(' Prior beta value ', exp_lambda)
start_date=datetime.datetime(2020,6,29)
end_date=start_date+datetime.timedelta(days=7)
posterior_data=cases.loc[(cases["Date"]>=start_date) & (cases["Date"]<end_date)].reset_index(drop=True)["Cases"]
exp1= len(posterior_data)
lambda1= np.sum(posterior_data)  
exp2= exp_lambda
lambda2=0
parameters = []
for i in range(4):
  print(start_date,end_date)
  exp2,lambda2 = generate_posterior(exp1,exp2,lambda1,lambda2)
  parameters.append([lambda2 + 1,exp2])
  start_date=end_date
  end_date=end_date+datetime.timedelta(days=7)
  posterior_data=cases.loc[(cases["Date"]>=start_date) & (cases["Date"]<end_date)].reset_index(drop=True)["Cases"]
  exp1 = len(posterior_data)
  lamda1 = np.sum(posterior_data)

#this function is to plot the gamma distribution from given parameters
def plot_gamma(parameters):
    for parameter in parameters:
        alpha, beta = parameter[0], parameter[1]
        x = np.linspace(gamma.ppf(0.01, alpha, scale=1/beta),gamma.ppf(0.99, alpha,scale = 1/beta), 100)
        MAP = (alpha)/beta
        plt.plot(x, gamma.pdf(x, alpha,scale=1/beta), label = 'MAP: %.4f ' %(MAP))
        plt.xlabel('X')
        plt.ylabel('Distribution')
    plt.legend()
    plt.show()

plot_gamma(parameters)

lambd= np.mean(june_data["Deaths"])
print(' MME of poisson distributed data ', lambd)
exp_lambda = 1/lambd 
print(' Prior beta value ', exp_lambda)
start_date=datetime.datetime(2020,6,29)
end_date=start_date+datetime.timedelta(days=7)
posterior_data=cases.loc[(cases["Date"]>=start_date) & (cases["Date"]<end_date)].reset_index(drop=True)["Deaths"]
exp1= len(posterior_data)
lambda1= np.sum(posterior_data)  
exp2= exp_lambda
lambda2=0

table = []
for i in range(4):
  print(start_date,end_date)
  exp2,lambda2 = generate_posterior(exp1,exp2,lambda1,lambda2)
  table.append([lambda2 + 1,exp2])
  start_date=end_date
  end_date=end_date+datetime.timedelta(days=7)
  posterior_data=cases.loc[(cases["Date"]>=start_date) & (cases["Date"]<end_date)].reset_index(drop=True)["Deaths"]
  exp1 = len(posterior_data)
  lamda1 = np.sum(posterior_data)

plot_gamma(table)

"""# Task D"""

#this function calculates the Mean Absolute Percentage Error
def MAPE(y_pred, y):
  mape = 0
  for x in range(0,len(y)):
      temp = (abs(y[x] - y_pred[x])/y[x])
      mape += temp
  return (mape/len(y))*100

# this function calculates the mean square error
def MSE(y_pred, y):
    error = 0
    for x in range(0,len(y)):
        error += (np.square(y_pred[x] - y[x]))
    return error/len(y)

def plot_graph(d,titlek):
  x=[i for i in range(len(d))]
  y1=d["Actual Value"]
  y2=d["Predicted Value"]
  mape=MAPE(y2,y1)
  mse=MSE(y2,y1)
  plt.plot(x,y1,label="Actual Values")
  plt.plot(x,y2,label="Predicted Values")
  dates=list(map(str,d["Date"]))
  dates=[i[:10] for i in dates]
  plt.xticks(x,dates,rotation="vertical")
  plt.xlabel("Date")
  plt.ylabel("Number of vaccines")
  titlek+=("\n MAPE = "+str(mape)+"%, MSE = "+str(mse))
  plt.title(label=titlek)
  plt.legend()
  plt.show()



"""## AutoRegression"""

# this function generates the data required for autoregression
def generate_data(data,n):
  data=list(data["Count"])
  X,y=[],[]
  for i in range(n,len(data)):
    arr=[1]
    arr+=[data[k] for k in range(i-1,i-n-1,-1)]
    y.append(data[i])
    X.append(arr)
  return np.array(X),np.array(y)

#this function gives the beta vector 
def get_beta(X,y):
  X_transpose=np.transpose(X)
  A=np.dot(X_transpose,X)
  B=np.dot(X_transpose,y)
  return np.linalg.solve(A,B)

#this function takes the data and n and performs AutoRegression(n)
def AutoRegression(data,n):
  X,y=generate_data(data,n)
  start,end=datetime.datetime(2021,5,1),datetime.datetime(2021,5,22)
  train_data=data.loc[(data["Date"]>=start) & (data["Date"]< end)]
  start,end=datetime.datetime(2021,5,22-n),datetime.datetime(2021,5,29)
  test_data=data.loc[(data["Date"]>=start )&( data["Date"]< end)]
  X_train,y_train=generate_data(train_data,n)
  X_test,y_test=generate_data(test_data,n)
  beta=get_beta(X_train,y_train)
  y_pred=[]
  for i in range(len(X_test)):
    t=np.array([1]+X_test[i])
    pred=np.dot(t,beta)
    t=np.reshape(t,(1,n+1))
    y_pred.append(pred)
    X_train=np.append(X_train,list(t),axis=0)
    y_train=np.append(y_train,y_test[i])
    beta=get_beta(X_train,y_train)
  d={"Date":list(test_data["Date"])[n:],"Actual Value":list(y_test),"Predicted Value":list(y_pred)}
  return pd.DataFrame(d)

"""### AR(3)"""

states={0:vaccination_alaska,1:vaccination_cali}
names={0:"Alaska",1:"California"}
values=[3,5]
for state in states:
  for value in values:
    d=AutoRegression(states[state],value)
    title="For state "+names[state]+" and n= "+str(value)
    print(d)
    plot_graph(d,title)
    print()

"""## EWMA"""

#This function performs Exponentially Weighted Moving Average
def EWMA(data,alpha):
  start,end=datetime.datetime(2021,5,21),datetime.datetime(2021,5,29)
  test_data=data.loc[(data["Date"]>=start )&( data["Date"]< end)]
  dates=list(test_data["Date"])
  y_test=list(test_data["Count"])
  y_pred=[y_test[0]]
  for i in range(1,len(y_test)):
    pred=(alpha*y_test[i])+(1-alpha)*y_pred[-1]
    y_pred.append(pred)
  d={"Date":dates[1:],"Actual Value":y_test[1:],"Predicted Value":y_pred[1:]}
  return pd.DataFrame(d)

states={0:vaccination_alaska,1:vaccination_cali}
names={0:"Alaska",1:"California"}
values=[0.5,0.8]
for state in states:
  for value in values:
    d=EWMA(states[state],value)
    title="For state "+names[state]+" and n= "+str(value)
    print(d)
    plot_graph(d,title)
    print()





"""# Task E"""

sep_start_date , sep_end_date = '2021-09-01', '2021-09-30'
nov_start_date, nov_end_date = '2021-11-01', '2021-11-30'
condition = (vaccination_alaska['Date'].dt.month==9) & (vaccination_alaska['Date'].dt.year==2021)
AL_sep_data = vaccination_alaska.loc[condition]
condition = condition = (vaccination_alaska['Date'].dt.month==11) & (vaccination_alaska['Date'].dt.year==2021)
AL_nov_data = vaccination_alaska.loc[condition]


AL_sep_data=pd.DataFrame(AL_sep_data)
AL_nov_data=pd.DataFrame(AL_nov_data)
AL_sep_data.reset_index(drop=True)

sep_start_date , sep_end_date = '2021-09-01', '2021-09-30'
nov_start_date, nov_end_date = '2021-11-01', '2021-11-30'
condition = (vaccination_cali['Date'].dt.month==9) & (vaccination_cali['Date'].dt.year==2021)
CA_sep_data = vaccination_cali.loc[condition]

condition = (vaccination_cali['Date'] >= nov_start_date) & (vaccination_cali['Date'] <= nov_end_date)
CA_nov_data = vaccination_cali.loc[condition]


CA_sep_data=pd.DataFrame(CA_sep_data)
CA_nov_data=pd.DataFrame(CA_nov_data)
CA_sep_data.reset_index(drop=True)

"""Paired T test for Sept 2021 for CA and AL"""

merge_sep = pd.merge(CA_sep_data, AL_sep_data, how='inner', on=['Date'])

delta_sept=abs(merge_sep['Count_x']-merge_sep['Count_y'])

import scipy.stats
def compute_paired_t_statistic(x):
  n=len(x)
  x_mean, x_var= np.mean(x), np.var(x)
  denom=np.sqrt(np.sum(np.square(x - x_mean))/(len(x)))
  t=(x_mean-0)/(denom/np.sqrt(n))
  print("u_hat: "+ str(x_mean))
  print("Standard error: "+ str(denom))
  print("T statistic: "+ str(t))
  return t
  
def t_test_paired(a,alpha):
  t=compute_paired_t_statistic(a)
  if abs(t)>scipy.stats.t.ppf(q=1-.05/2,df=28):
    return False
  else:
    return True

t_test_paired(delta_sept,0.05)

"""Paired T test for November 2021 for CA and AL"""

merge_nov = pd.merge(CA_nov_data, AL_nov_data, how='inner', on=['Date'])

delta_nov=abs(merge_nov['Count_x']-merge_nov['Count_y'])

t_test_paired(delta_nov,0.05)

"""## Result of T 2 sampled Paired testing for mean of no:of vaccines administered  in CA/Al:

Null hypothesis (H0):

Mean of Sept'21 vaccines administered in CA/Al = Mean of Nov'21 vaccines administered

Alternate hypothesis(H1):

Mean of Sept'21 vaccines administered in CA/Al is different from Mean of Nov'21 vaccines administered.

Procedure :

We have taken the alpha = 0.05 n=30 as given in Task and calculated the numerator and denominator of T value in the above t_test_paired function .
Result:

As the T statistic value for vaccines administered in CA/Al around September'21 = 13.184 which is greater than 2.45 we are rejecting the NULL hypothesis.

As the T statistic value for vaccines administered in CA/Al around Nov'21 = 7.68 which is greater than 2.45 we are rejecting the NULL hypothesis.

Is the paired T Test appliable ?

Paired T-Test Assumes that the data X and Y are dependent and diff of(X,Y) data set is normally distributed and as these two assumptions are failing Paired T test is not applicable.

# Exploratory Data Analysis - 1

1.Use your X dataset to check if COVID19 cases/vaccinations had an impact on the X data. State your hypothesis clearly and determine the best tool (from among those learned in class) to apply to your hypotheses. Also, check whether the tool/test is applicable or not.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
from scipy.stats import gamma
import math
import datetime
import warnings
import random
from itertools import permutations
from sklearn.linear_model import LinearRegression
warnings.filterwarnings("ignore")

total_cases=pd.read_csv("/content/drive/MyDrive/Prob Stats Project Data/cases.csv")#cases dataset
total_vaccinations=pd.read_csv("/content/drive/MyDrive/Prob Stats Project Data/vaccinations.csv")#vaccination dataset
data=pd.read_csv("/content/drive/MyDrive/Prob Stats Project Data/JNJ stock Data.csv")#x dataset

total_vaccinations.rename(columns={"Location":"state"},inplace=True)#renaming the column from location to state in vaccinations file
total_cases.rename(columns={"submission_date":"Date"},inplace=True)#renaming the columns from submission_date to date in cases file

def filter_data(data,states,columns):#filtering the data
  data=data.loc[data["state"].isin(states)]
  return data[columns].reset_index(drop=True)

states=["AL","CA"]#choosing for only the states allocated
columns=["Date","state","new_case","new_death","tot_cases"]
cases=filter_data(total_cases,states,columns)
columns=["Date","state","Administered","Admin_Per_100K"]
vaccinations=filter_data(total_vaccinations,states,columns)

def remove_nulls(data):#removing null values
  data=data[data.select_dtypes(include=[np.number]).ge(0).all(1)]
  return data.dropna(axis=0,how="any")
def convert_date(data):#converting the date format
  data['Date'] =  pd.to_datetime(data['Date'], format=r"%m/%d/%Y")
  return data
def sort_seperate_states(data):#sorting for the assigned states
  a,b=data.loc[data["state"]=="AL"],data.loc[data["state"]=="CA"]
  a.sort_values(by="Date",ignore_index=True,inplace=True)
  b.sort_values(by="Date",ignore_index=True,inplace=True)
  return a,b
def getMonth(data):
  data["month"]=pd.DatetimeIndex(data['Date']).month
  return data
def remove_outliers(value,min_threshold,max_threshold):#removing the outliers
  if value < min_threshold or value > max_threshold:
    return np.nan
  return value

def remove_all_outliers(data,columns):
  for column in columns:
    temp_data=data[column]
    q1=np.percentile(temp_data,25)
    q3=np.percentile(temp_data,75)
    iqr=q3-q1
    min_threshold,max_threshold=q1-(1.5*iqr),q3+(1.5*iqr)
    data[column] = data.apply(lambda x: remove_outliers(x[column],min_threshold, max_threshold), axis=1)
  return remove_nulls(data).reset_index(drop=True)

def get_daily_data(data):
  data.sort_values(by="Date",ignore_index=True,inplace=True)
  t1=data["Administered"][0]
  t2=data["Admin_Per_100K"][0]
  data['Count'] = data.Administered - data.Administered.shift(1)
  data["Count_Per_100K"]=data.Admin_Per_100K-data.Admin_Per_100K.shift(1)
  data["Count"][0]=t1
  data["Count_Per_100K"]=t2
  return data

def transform_data(data):
  data=remove_nulls(data)
  data=remove_all_outliers(data,["new_case","new_death"])
  data=convert_date(data)
  data=getMonth(data)
  data_alaska,data_cali=sort_seperate_states(data)
  return data_alaska,data_cali

def convert(row):
  date=str(row["Date"])
  if "-" in date:
    date=pd.to_datetime(date,format=r"%d-%m-%Y")
  else:
    date=pd.to_datetime(date,format=r"%m/%d/%Y")
  return date
def remove_commas(row):
  for column in ["Volume"]:
    row[column]=row[column].replace(",","")
  return row

data["Date"]=data.apply(convert,axis=1)
data=data.apply(remove_commas,axis=1)
for column in ["Open","High","Low","Close"]:
  data[column]=pd.to_numeric(data[column])

cases_alaska,cases_cali=transform_data(cases)

def transform_data_Vacc(data):
  data=remove_nulls(data)
  data=convert_date(data)
  data_alaska,data_cali=sort_seperate_states(data)
  data_alaska=get_daily_data(data_alaska)
  data_cali=get_daily_data(data_cali)
  data_alaska=remove_all_outliers(data_alaska,["Count","Count_Per_100K"])
  data_cali=remove_all_outliers(data_cali,["Count","Count_Per_100K"])
  return data_alaska,data_cali

vaccination_alaska,vaccination_cali=transform_data_Vacc(vaccinations)

data=data.apply(remove_commas,axis=1)
for column in ["Open","High","Low","Close","Volume"]:
  data[column]=pd.to_numeric(data[column])


def remove_nulls(data):
  data=data[data.select_dtypes(include=[np.number]).ge(0).all(1)]
  return data.dropna(axis=0,how="any")

def remove_outliers(value,min_threshold,max_threshold):
  if value < min_threshold or value > max_threshold:
    return np.nan
  return value

def remove_all_outliers(data,columns):
  outlier_index = []
  for column in columns:
    temp_data=data[column]
    print(temp_data)
    q1=np.percentile(temp_data,25)
    q3=np.percentile(temp_data,75)
    iqr=q3-q1
    min_threshold,max_threshold=q1-(1.5*iqr),q3+(1.5*iqr)
    #Determine a list of indices of outliers for feature col
    outlier_list_col = data[(data[column] > max_threshold) | (data[column] < min_threshold)]
    print("Outliers in Column "+str(column) + " : " + str(len(outlier_list_col)))
    data[column] = data.apply(lambda x: remove_outliers(x[column],min_threshold, max_threshold), axis=1)
    plt.scatter(data['Date'],data[column])
    plt.scatter(outlier_list_col['Date'],outlier_list_col[column])
    plt.rcParams["figure.figsize"] = (10,6)
    plt.show()
  return remove_nulls(data).reset_index(drop=True)

data=remove_all_outliers(data,["Close","Open","High","Low","Volume"])
print(data)



dataframe=data.merge(cases_alaska, how='inner', on='Date')#performing merging on date column where state is Alaska
dataframe.head(5)

dataframe_cali=data.merge(cases_cali, how='inner', on='Date')#performing merging on date column where state is California
dataframe_cali.head(5)

"""##Pearson test"""

dataframe_alaska=dataframe.sort_values(by=['Date'])
print("** total cases plot**" )
#plt.figure(figsize=(15,10))
plt.plot(dataframe_alaska['Date'], dataframe_alaska['tot_cases'])
plt.xlabel('Date')
plt.ylabel('total cases')
plt.show()
print("**JNJ Stock market**" )
#plt.figure(figsize=(15,10))
plt.plot(dataframe_alaska['Date'], dataframe_alaska['Close'])
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.show()

handle = 'tot_cases'
test_handle = 'Close'

stocks_close_price_mean = np.mean(dataframe_alaska[test_handle])
covid_total_confirmed_mean = np.mean(dataframe_alaska[handle])
numer = 0
denom_x = 0 
denom_y = 0 
for i in range(dataframe_alaska.shape[0]):
  x_minus_Xavg = np.array(dataframe_alaska[test_handle])[i] - stocks_close_price_mean
  y_minus_Yavg = np.array(dataframe_alaska[handle])[i] - covid_total_confirmed_mean
  numer = numer + (x_minus_Xavg * y_minus_Yavg)
  denom_x =  denom_x + np.square(x_minus_Xavg)
  denom_y =  denom_y + np.square(y_minus_Yavg)

coeff = numer / np.sqrt(denom_x * denom_y)
print("Alaska:- ",coeff)
#as coeff is > 0.5 . They are positively correlated

dataframe_cali=dataframe_cali.sort_values(by=['Date'])
print("** Covid new cases Data**" )
#plt.figure(figsize=(15,10))
plt.plot(dataframe_cali['Date'], dataframe_cali['tot_cases'])
plt.xlabel('Date')
plt.ylabel('new cases')
plt.show()

print("** Stock market Data**" )
#plt.figure(figsize=(15,10))
plt.plot(dataframe_cali['Date'], dataframe_cali['Close'])
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.show()

handle = 'tot_cases'
test_handle = 'Close'

stocks_close_price_mean = np.mean(dataframe_cali[test_handle])
covid_total_confirmed_mean = np.mean(dataframe_cali[handle])
numer = 0
denom_x = 0 
denom_y = 0 
for i in range(dataframe_cali.shape[0]):
  x_minus_Xavg = np.array(dataframe_cali[test_handle])[i] - stocks_close_price_mean
  y_minus_Yavg = np.array(dataframe_cali[handle])[i] - covid_total_confirmed_mean
  numer = numer + (x_minus_Xavg * y_minus_Yavg)
  denom_x =  denom_x + np.square(x_minus_Xavg)
  denom_y =  denom_y + np.square(y_minus_Yavg)

coeff = numer / np.sqrt(denom_x * denom_y)
print("California:- ",coeff)
#as coeff is > 0.5 . They are positively correlated

"""##K-S Test

"""

def eCDF_value(state1):
    state_1 = len(state1)
    listvalues = sorted(state1)
    value=1/(state_1)
    X = []
    f_x = []
    cdf=value

    for i in range(0,state_1):
        X = X + [listvalues[i]]
        f_x = f_x + [cdf]
        cdf+=value

    return X,f_x

def ks_test(state1,state2,X1, f_X1, X2, f_X2):
  i=0
  j=0
  maxi1=0
  while i<len(state1) and j<len(state2):
    if state1[i] < state2[j]:
      if j==0:
        F_x_cap=0
      else:
        F_x_cap = f_X2[X2.index(state2[j-1])]
      if i==0:
        F_y_left=0
      else:
        F_y_left = f_X1[X1.index(X1[i-1])]
      if i==len(X1)-1:
        F_y_right=1
      else:
        F_y_right=f_X1[X1.index(state1[i])]
      maxi_temp=max(abs(F_x_cap-F_y_left),abs(F_x_cap-F_y_right))
      if maxi_temp>maxi1:
        maxi1=maxi_temp
        index=i
      i+=1
    else:
      j+=1

  print("K-S statistic is " + str(maxi1)+" and point of max difference is "+str(state1[index]))
  return maxi1

handle = 'tot_cases'
test_handle = 'Close'
handle_data = dataframe_cali['tot_cases']
test_handle_data = dataframe_cali[test_handle]
X1, f_X1 = eCDF_value(handle_data)
X2, f_X2 = eCDF_value(test_handle_data)
KS_p_value_deaths = ks_test(handle_data.tolist(), test_handle_data.tolist(),X1, f_X1, X2, f_X2)

handle = 'tot_cases'
test_handle = 'Close'
handle_data = dataframe_alaska['tot_cases']
test_handle_data = dataframe_alaska[test_handle]
X1, f_X1 = eCDF_value(handle_data)
X2, f_X2 = eCDF_value(test_handle_data)
KS_p_value_deaths = ks_test(handle_data.tolist(), test_handle_data.tolist(),X1, f_X1, X2, f_X2)

"""##Permutation Test

"""

def permutation_test_function(X, Y, n=5000, threshold=0.05):
    T_obs = abs(np.mean(X) - np.mean(Y)) 
    xy = np.append(X,Y)
    p_value = 0.0
    for i in range(n):
        permutation = np.random.permutation(xy)
        X1 = permutation[:len(X)]
        Y1 = permutation[len(X):]
        Ti = abs(np.mean(X1) - np.mean(Y1))
#         print(Ti, T_obs)
        if(Ti > T_obs):
            p_value += 1.0
    
#     p_value = p_value/float(np.math.factorial(n))
    p_value = p_value/n
    print("The p-value is: ", p_value)
    if(p_value <= threshold):
        print("==> Reject the Null Hypothesis")
    else:
        print("==> Accept the Null Hypothesis")
    return

dataframe_alaska['tot_cases_abs'] = abs(dataframe_alaska['tot_cases'])
dataframe_alaska['Close_abs'] = abs(dataframe_alaska['Close'])

total_cases_abs = dataframe_alaska['tot_cases_abs'].tolist()
close_abs = dataframe_alaska['Close_abs'].tolist()

print("--------For Alaska--------")
permutation_test_function(total_cases_abs, close_abs)

dataframe_cali['tot_cases_abs'] = abs(dataframe_cali['tot_cases'])
dataframe_cali['Close_abs'] = abs(dataframe_cali['Close'])

total_cases_cali_abs = dataframe_cali['tot_cases_abs'].tolist()
close_abs = dataframe_cali['Close_abs'].tolist()

print("--------For Cali--------")
permutation_test_function(total_cases_cali_abs, close_abs)



"""# Exploratory Analysis - 2"""

total_cases=pd.read_csv("/content/drive/MyDrive/Prob Stats Project Data/cases.csv")
total_cases.rename(columns={"submission_date":"Date"},inplace=True)

def clean_data(data):
  data=remove_nulls(data)
  data=convert_date(data)
  return data

total_cases=clean_data(total_cases)

def print_(data,filter):
  cases=data[["Date",filter]]
  cases=cases.sort_values(by="Date").reset_index(drop=True)
  cases=cases.groupby("Date").sum()
  x=[i for i in range(len(cases))]
  y=list(cases[filter])
  # plt.figure(figsize=(12,5))
  plt.plot(x,y)
  plt.xlabel("Days")
  s="Cases" if filter=="new_case" else "Deaths"
  plt.ylabel("Number of "+s)
  plt.title("Number of {} each day".format(s))
  plt.show()

def compute_weekly(data,filter):
  cases=data[["Date",filter]]
  cases=cases.sort_values(by="Date").reset_index(drop=True)
  cases=cases.groupby("Date").sum()
  cases=cases.reset_index(drop=True)
  cases["Day"]=[i for i in range(1,len(cases)+1)]
  cases=cases[filter].groupby((cases['Day'] - 1) // 7).sum().reset_index(drop=True)
  return list(cases)

def print_weekly(data,filter):
  data=compute_weekly(data,filter)
  x=[i for i in range(len(data))]
  plt.plot(x,data)
  plt.xlabel("Weeks")
  s="Cases" if filter=="new_case" else "Deaths"
  plt.ylabel("Number of "+s)
  plt.title("Number of {} each week".format(s))
  plt.show()

def print_lockdown(data,day,filter):
  cases=data[["Date",filter]]
  cases=cases.sort_values(by="Date").reset_index(drop=True)
  cases=cases.groupby("Date").sum()
  x=[i for i in range(day)]
  y=list(cases[filter])
  plt.plot(x,y[:day],label="Before Lockdown")
  x=[i for i in range(day,day+100)]
  plt.plot(x,y[day:day+100],label="After Lockdown")
  plt.xlabel("Days")
  s="Cases" if filter=="new_case" else "Deaths"
  plt.ylabel("Number of "+s)
  plt.title("Number of {} each day".format(s))
  plt.legend()
  plt.show()

def print_waves(data,waves,filter):
  cases=data[["Date",filter]]
  cases=cases.sort_values(by="Date").reset_index(drop=True)
  cases=cases.groupby("Date").sum()
  x=[i for i in range(len(cases))]
  y=list(cases[filter])
  plt.figure(figsize=(14,5))
  plt.plot(x,y)
  for i,d in enumerate(waves):
    if i%2==0:
      color="b"
      t="Start of the Wave"
    else:
      color="r"
      t="End of the Wave"
    plt.axvline(x=d,ymin=0.03,ymax=0.6,color=color,linestyle='--',linewidth=1.5,label=t)
    if i<2:
      plt.legend()
  plt.xlabel("Days")
  s="Cases" if filter=="new_case" else "Deaths"
  plt.ylabel("Number of "+s)
  plt.title("Highlighting the waves")
  plt.show()

def generate_data(l,a,b,T_obs):
  print("T_obs",T_obs)
  def find_xmean(row):
    return sum(row["X"])/len(row["X"])
  def find_ymean(row):
    return sum(row["Y"])/len(row["Y"])
  def finddiff(row):
    return round(abs(row["X_mean"]-row["Y_mean"]),2)
  def solve(row):
    return 1 if row["T = |X_mean - Y_mean |"] > T_obs else 0
  n=len(l)
  X,Y=[],[]
  for i in range(n):
    X.append(l[i][:a])
    Y.append(l[i][a:])
  data=pd.DataFrame()
  
  data["X"],data["Y"]=X,Y
  data["X_mean"] = data.apply(find_xmean, axis = 1)
  data["Y_mean"] = data.apply(find_ymean, axis = 1)
  data["T = |X_mean - Y_mean |"] = data.apply(finddiff,axis = 1)
  data["I (T > T_obs)"] = data.apply(solve,axis = 1)
  return data

def perm_generator(seq):
  seen = set()
  length = len(seq)
  while True:
      perm = tuple(random.sample(seq, length))
      if perm not in seen:
          seen.add(perm)
          yield perm   
def p_value(data):
  return data.iloc[:,-1].sum()/len(data)
def get_p_value(data_alaska,data_cali,n):
  a,b=len(data_alaska),len(data_cali)
  data=data_alaska+data_cali
  rand_perms = perm_generator(data)
  permlist= [next(rand_perms) for _ in range(n)]
  permlist=list(map(list,permlist))
  T_obs = abs(np.mean(data_alaska)-np.mean(data_cali))
  data=generate_data(permlist,a,b,T_obs)
  return p_value(data)
def permutation_test(X,Y,alpha):
  a,b = len(X),len(Y)
  n=min(math.factorial(a+b),10000)
  p=get_p_value(X,Y,n)
  print("P Value:",p)
  if p < alpha :
    return False
  else:
    return True

"""## CASES"""

filter="new_case"

print_(total_cases,filter)

print_weekly(total_cases,filter)

print_lockdown(total_cases,80,filter)

waves=[300,450,570,650,710,800]
print_waves(total_cases,waves,filter)

start1,end1,start2,end2=300,450,710,800
cases=total_cases[["Date","new_case"]]
cases=cases.sort_values(by="Date").reset_index(drop=True)
cases=cases.groupby("Date").sum()

dist1=list(cases["new_case"])[start1:end1]
dist2=list(cases["new_case"])[start2:end2]

permutation_test(dist1,dist2,0.05)

"""## Deaths"""

filter="new_death"

print_(total_cases,filter)

print_weekly(total_cases,filter)

print_lockdown(total_cases,80,filter)

waves=[300,450,570,660,730,800]
print_waves(total_cases,waves,filter)

start1,end1,start2,end2=300,450,710,800
cases=total_cases[["Date","new_case"]]
cases=cases.sort_values(by="Date").reset_index(drop=True)
cases=cases.groupby("Date").sum()

dist1=list(cases["new_case"])[start1:end1]
dist2=list(cases["new_case"])[start2:end2]

permutation_test(dist1,dist2,0.05)



"""Deaths and Cases"""

data=total_cases[["Date","new_case","new_death"]]

data=clean_data(data)

data=data.sort_values(by="Date").reset_index(drop=True)



data=data.groupby("Date").sum()

permutation_test(list(data["new_case"]),list(data["new_death"]),0.05)

cases=total_cases[["Date","new_case","new_death"]]

cases=total_cases[["Date","new_case","new_death"]]
cases=cases.sort_values(by="Date").reset_index(drop=True)
cases=cases.groupby("Date").sum()

x=[i for i in range(1,len(cases)+1)]
y1=list(cases["new_case"])
y2=list(cases["new_death"])
y=[y1[i]/y2[i] if y2[i]!=0 else 0 for i in range(len(x))]
plt.plot(x,y)

